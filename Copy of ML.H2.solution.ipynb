{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of ML.H2.solution.ipynb","provenance":[{"file_id":"181WkWwUF3Q9LnSaWKDOnopwAtNwltZXH","timestamp":1611170765507}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"F24EmvTrGjHc"},"source":["# **Homework Assignment #2 Solution**\n","\n","Assigned: February 1, 2021\n","\n","Due: February 19, 2021\n","\n","\n","\n","---\n","\n","This assignment consists of questions that require a short answer and one Python programming task. You can enter your answers and your code directly in a Colaboratory notebook and upload the **shareable** link for your notebook as your homework submission.\n","\n","\n","---\n","\n","#1.\n","\n","(10 points) Consider training set accuracy and test set accuracy curves\n","plotted below as a function of the number of nodes in a decision tree.\n","While this graph plots accuracy, we can also compute error as 1.0 - accuracy.\n","\n","![alt text](https://drive.google.com/uc?id=1ScPyMBFemm6dbgu1saUqSV3dJdUlwIdd)\n","\n","Can you suggest a way to determine the amount of overfit in the learned model\n","based on these curves? Explain / justify your answer.\n","\n","---\n","\n","Answer: Training accuracy minus test accuracy provides an estimate of the\n","amount of overfitting.\n","\n","---\n","\n","Based on the curve in the graph, what size decision tree would you choose to use and why?\n","\n","---\n","\n","Answer: Size 10 because this tree has the highest accuracy on test data.\n","\n","---\n","\n","#2.\n","\n","(10 points) To demonstrate your understanding of k-nearest neighbors, construct a labeled dataset where the dimensionality is 1 and the leave-one-out cross-validation accuracy for 1-nearest neighbor is always 0. As a reminder, leave-one-out uses all of the training data except one instance for learning the model and uses the held-out instance for testing, repeating the process for each possible holdout point and averaging the results. Therefore, this describes a situation where the classifier always gets the prediction wrong. \n","\n","---\n","\n","Answer: Plot points on real number line with labels that alternate between\n","$+$ and $-$. In leave-one-out cross validation we compute the predicted class for\n","each point given all the remaining points. Because the neighbors of every point\n","are in the opposite class, leave-one-out cross validation predictions will\n","never be correct.\n","\n","---\n","\n","#3.\n","\n","(20 points) Consider training a perceptron using the datapoints in the table below, presented in this order.\n","\n","Instance | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8\n","--- | --- | --- | --- | --- | --- | --- | --- | ---\n","Label $y$ | +1 | -1 | +1 | -1 | +1 | -1 | +1 | +1\n","Data $(x_1,x_2)$ | 10, 10 | 0, 0 | 8, 4 | 3, 3 | 4, 8 | 0.5, 0.5 | 4, 3 | 2, 5\n","\n","Given an initial set of weights $w = (1, 1)$ and bias $b=0$, show each step of the perceptron algorithm for the above sequence of instances over one epoch. This includes computation of the activation and adjustment of the weights after each instance. \n","\n","What is the accuracy of the perceptron after this first epoch?\n","\n","Will this perceptron eventually converge on a model with zero error for the training data? Why or why not?\n","\n","---\n","\n","Answer:\n","\n","$x_1$ | $x_2$ | $y$ | $w_1$ | $w_2$ | $b$ | $a$ | $ya$\n","--- | --- | --- | --- | --- | --- | --- | ---\n","10 | 10 | 1 | 1 | 1 | 0 | 20 | 20 ($>0$)\n","0 | 0 | -1 | 1 | 1 | 0 | 0 | 0 ($<=0$, need to adjust)\n","8 | 4 | 1 | 1 | 1 | -1 | 11 | 11 ($>0$)\n","3 | 3 | -1 | 1 | 1 | -1 | 5 | -5 ($<=0$, need to adjust)\n","4 | 8 | 1 | -2 | -2 | -2 | -26 | -26 ($<=0$, need to adjust)\n","0.5 | 0.5 | -1 | 2 | 6 | -1 | 3 | -3 ($<=0$, need to adjust)\n","4 | 3 | 1 | 1.5 | 5.5 | -2 | 20.5 | 20.5 ($>0$)\n","2 | 5 | 1 | 1.5 | 5.5 | -2 | 28.5 | 28.5 ($>0$)\n","\n","The accuracy is 0.5 (50%).\n","\n","As the graph shows (+1 examples in red, -1 examples in blue), the data is linearly separable. Thus, the perceptron should eventually converge.\n","\n","![alt text](https://drive.google.com/uc?id=1pRZx9iBo1G-DKOhNtbc_YuQ0sxMVV1ds)\n","\n","---\n","\n","\n","\n","---\n","\n","#4.\n","\n","(80 points) In this programming task you will gain familiarity with k-nearest neighbor classification, the sklearn machine learning library, and working with a handwriting recognition dataset.\n","\n","For this program, compare the accuracy of four classifiers for correctly classifying the hand-written number from the digits dataset available as a sklearn library (see https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html for details). The classifiers are:\n","\n","- Decision tree (you can use the sklearn library for this)\n","- K nearest neighbors with 5 neighbors (you can use the sklearn library for this)\n","- Majority classifier (you can use the sklearn library for this)\n","- Your own implementation of a KNN classifier (do not use the sklearn library for this). This classifier should compute Euclidean distance between pairs of points and take the number of neighbors to consider as a parameter.\n","\n","To report performance, randomly select 2/3 of the data points to use for training and 1/3 to use for testing. Repeat 3 times and report accuracy results averaged over the 3 trials. Compare accuracy results for the classifiers. For your KNN implementation, try different values for $k$ including 1, 3, 5, 7, and 9. Argue which value of $k$ you would choose and why."]},{"cell_type":"code","metadata":{"id":"3dq-e4mGlxPg","colab":{"base_uri":"https://localhost:8080/","height":444},"executionInfo":{"status":"ok","timestamp":1590184379106,"user_tz":420,"elapsed":117537,"user":{"displayName":"Diane Cook","photoUrl":"","userId":"17411349131353838798"}},"outputId":"5c771e5a-7dcb-44d1-fb75-e85277ae2809"},"source":["# HW2 Problem 4 Solution\n","\n","import time\n","import matplotlib.pyplot as plot\n","import numpy as np\n","from sklearn.datasets import load_digits\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.dummy import DummyClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","import matplotlib.pyplot as plt\n","from math import sqrt\n","\n","\n","\n","def ComputeAccuracy(ytest, newlabels):\n","  numright = 0\n","  n = len(ytest)\n","  for i in range(n):\n","    if newlabels[i] == ytest[i]:\n","      numright += 1\n","  return float(numright) / float(n)\n","\n","\n","def euclidean_distance(x, y):\n","  distance = 0.0\n","  for i in range(len(x)-1):\n","    distance += (x[i] - y[i])**2\n","  return sqrt(distance)\n","\n","\n","def get_neighbors(train, test, n_neighbors):\n","  distances = list()\n","  n = len(train)\n","  for i in range(n):\n","    dist = euclidean_distance(train[i], test)\n","    distances.append((i, dist))\n","  distances.sort(key=lambda x: x[1])\n","  neighbors = list()\n","  for i in range(n_neighbors):\n","    neighbors.append(distances[i][0])\n","  return neighbors\n","\n","\n","def KNN(xtrain, xtest, ytrain, ytest, n_neighbors):\n","  n_test = len(ytest)\n","  numright = 0\n","  for i in range(n_test):\n","    neighbors = get_neighbors(xtrain, xtest[i], n_neighbors)\n","    output_values = ytrain[neighbors]\n","    counts = np.bincount(output_values)\n","    prediction = np.argmax(counts)\n","    if prediction == ytest[i]:\n","      numright += 1\n","  return float(numright) / float(n_test)\n","\n","\n","if __name__ == \"__main__\":\n","  t0 = time.time()\n","  digits = load_digits()\n","  plt.gray()\n","  plt.matshow(digits.images[0])\n","  plt.show()\n","\n","  n = len(digits.images)\n","  data = digits.images.reshape((n, -1))\n","\n","  classifiers = [\n","     (DummyClassifier(strategy='most_frequent'), \"Majority\"),\n","     (DecisionTreeClassifier(), \"DT\"),\n","     (KNeighborsClassifier(n_neighbors=5), \"5NN\")\n","  ]\n","\n","  for clf, name in classifiers:\n","    print(\"classifier\", name)\n","    results = []\n","    for i in range(3):\n","      xtrain, xtest, ytrain, ytest = \\\n","        train_test_split(data, digits.target, test_size=0.33, random_state = i)\n","      clf.fit(xtrain, ytrain)\n","      newlabels=clf.predict(xtest)\n","      results.append(ComputeAccuracy(ytest, newlabels))\n","    print(np.mean(results))\n","\n","  print(\"Our KNN Classifier (k=1)\")\n","  results = []\n","  for i in range(3):\n","    xtrain, xtest, ytrain, ytest = \\\n","      train_test_split(data, digits.target, test_size=0.33, random_state = i)\n","    knn_results = KNN(xtrain, xtest, ytrain, ytest, 3)\n","    results.append(knn_results)\n","  print(np.mean(results))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 288x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["classifier Majority\n","0.08585858585858586\n","classifier DT\n","0.8540965207631874\n","classifier 5NN\n","0.978675645342312\n","Our KNN Classifier (k=1)\n","0.9831649831649831\n"],"name":"stdout"}]}]}